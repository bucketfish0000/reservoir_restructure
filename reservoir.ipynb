{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LORENZ TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import ReservoirModel\n",
    "import utils\n",
    "from utils import integration_lorenz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"lorenz.config\"  # FIXME: Maybe use YAML? If you want hard-typed config, use pydantic\n",
    "with open(config_path) as config_file:\n",
    "    config = json.load(config_file, object_pairs_hook=OrderedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "# TODO: Maybe make a cached data file\n",
    "data, time = integration_lorenz(\n",
    "    epoch=config[\"system\"][\"length\"],\n",
    "    delta_t=config[\"system\"][\"d_t\"]\n",
    ")\n",
    "print(f\"{data.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_lorenz = ReservoirModel(config)\n",
    "loss, loss_before_training = RP_lorenz.training(data)\n",
    "print(f\"{loss_before_training=:.04f}\")\n",
    "print(f\"{loss=:.04f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lorenz = RP_lorenz.run_by_self(data)\n",
    "#forced_output_lorenz, _ = RP_lorenz.run_with_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import importlib\n",
    "import utils\n",
    "\n",
    "importlib.reload(utils)\n",
    "# utils.plot_time_sequence(RP_lorenz.training_time,RP_lorenz.training_time+1000,RP_lorenz.training_time,RP_lorenz.f,torch.tensor(output_lorenz),torch.tensor(lorenz).T,time,3)\n",
    "lorenz_time = np.linspace(0, RP_lorenz.run_time * RP_lorenz.d_t, RP_lorenz.run_time)\n",
    "\n",
    "utils.plot_time_sequence(\n",
    "    RP_lorenz.training_time - 2000,\n",
    "    RP_lorenz.training_time + 1000,\n",
    "    RP_lorenz.training_time,\n",
    "    torch.tensor(output_lorenz),\n",
    "    torch.tensor(data),\n",
    "    lorenz_time,\n",
    "    3,\n",
    ")\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig2=plt.figure()\n",
    "# fig2.set_figwidth(40)\n",
    "# fig2.set_figheight(15)\n",
    "\n",
    "# print(output[0])\n",
    "# plt.subplot(311)\n",
    "# plt.plot(time[1000:1200],lorenz_x[1000:1200])\n",
    "# plt.plot(time[1000:1200],(output.T[0][1005-10:1205-10]))\n",
    "# #plt.plot(time[1000:1200],forced_output.T[0][1005:1205])\n",
    "# plt.subplot(312)\n",
    "# plt.plot(time[1000:1200],lorenz_y[1000:1200])\n",
    "# plt.plot(time[1000:1200],(output.T[1][1005-10:1205-10]))\n",
    "# #plt.plot(time[1000:1200],forced_output.T[1][1005:1205])\n",
    "# plt.subplot(313)\n",
    "# plt.plot(time[1000:1200],lorenz_z[1000:1200])\n",
    "# plt.plot(time[1000:1200],(output.T[2][1005-10:1205-10]))\n",
    "# #plt.plot(time[1000:1200],forced_output.T[2][1005:1205])\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACKEY-GLASS TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "importlib.reload(utils)\n",
    "\n",
    "sequence, time = utils.discrete_mackey_glass(epoch=6000)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(40)\n",
    "fig.set_figheight(10)\n",
    "plt.plot(time, sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "\n",
    "importlib.reload(model)\n",
    "\n",
    "RP_mg = model.reservoirModel(\"mackey_glass.config\")\n",
    "print(\"W_out before_training:\\n\", RP_mg.W_out)\n",
    "RP_mg.training()\n",
    "print(\"W_out after_training:\\n\", RP_mg.W_out)\n",
    "\n",
    "output_mg, _ = RP_mg.run_by_self()\n",
    "\n",
    "forced_output_mg, _ = RP_mg.run_with_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "importlib.reload(utils)\n",
    "# plot of train-and-auto-run\n",
    "utils.plot_time_sequence(\n",
    "    RP_mg.training_time - 2000,\n",
    "    RP_mg.training_time + 2000,\n",
    "    RP_mg.training_time,\n",
    "    RP_mg.f,\n",
    "    torch.tensor(output_mg),\n",
    "    torch.tensor(sequence).T,\n",
    "    time,\n",
    "    1,\n",
    ")\n",
    "# plotting a clip from output generated by continued input into trained model\n",
    "utils.plot_time_sequence(\n",
    "    RP_mg.training_time - 2000,\n",
    "    RP_mg.training_time + 2000,\n",
    "    RP_mg.training_time,\n",
    "    RP_mg.f,\n",
    "    torch.tensor(forced_output_mg),\n",
    "    torch.tensor(sequence).T,\n",
    "    time,\n",
    "    1,\n",
    ")\n",
    "\n",
    "\n",
    "# utils.plot_time_sequence(0,RP.run_time,RP.f,p,s,time,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
